# Upwork.com Analysis  
* use by Excel sheet


당신은 어머니를 사랑합니까?
아마도 사랑할것입니다. 저 역시 어머니를 몹시 존경하고 사랑합니다.  저의 어머니는 나에게 항상 사람은 근면하고 성실해야 하며 선택을 잘해야 한다고 말합니다.
인생은 선택의 련속이 아닙니까? 선택 그자체는 일종의 모험도 동반합니다. 
만약 당신이 나를 선택한다면 그것 역시 모험일지도 모릅니다. 그러나 안심하십시오. 당신의 선택은 정확할것입니다. 저를 선택함으로 하여 당신은 당신이 해결하고 싶어하는 분야들에서 근면하고 책임적인 능력있는 개발자를 알게되는 행운을 나는 독특한 착상과 기술을 지닌, 개발자를 파악하고 등용할줄아는 관리형의 인재를 알게되는 행운을 지니게 될것입니다.  
우연은 노력하는 사람에게 운명이 놓아주는 다리라고 합니다. 당신과 만나게 되는것이 운명의 계기점이 되는지도 모르겠습니다...
이 프로젝트에서 가장 중요한것은 바로 ㅇㅇㅇ를 해결하는것입니다.
나는 이와 같은 형식의 과제를 이미 경험하였고 자산들도 있습니다.
나는 나의 결과물에 대해 언제나 책임지며 당신에게 최상의 만족을 주기 위해 노력할것입니다.
나는 비록 당신이 나를 선택하지 않아도 능력있고 경험이 풍부한 개발자를 만나 이번 프로젝트가 성공적으로 끝나기를 진심으로 바랍니다.
Do you love your mother?
Probably. I also respect and love my mother very much. My mother always tells me that people should be diligent, honest, and make good choices.
Isn't life a series of choices? Choice itself is also an adventure.

If you choose me, it may be an adventure too. But rest assured(Don't worry), your choice will be correct. By choosing me, you will be fortunate to meet a diligent, responsible, and capable developer in the fields you want to solve, and I will be fortunate to meet a managerial talent with unique ideas and skills who knows how to identify and hire developers.

They say that coincidence is a bridge that fate gives to those who try hard. Our meeting might be a turning point in our destiny...
Let's discuss the assignment.
The most important thing in this project is to solve ㅇㅇㅇ.
I have already experienced this type of work and have proven assets.
I will always be responsible for my results and strive to provide the best satisfaction to you. And for my mother. I sincerely hope that even if you don't choose me, you will find a capable and experienced developer and that this project will be a success.
Thank you for your time.
Take Care
마우스 클릭한번으로 이어지는 행운
우리가 알게되는것이 운명의 계기점으로 될것입니다.
만약 당신이 나를 선택한다면 그것은 일종의 모험일지도 모릅니다. 그러나 안심하십시오. 당신의 선택은 정확할것입니다.
저를 선택함으로 하여 당신은 당신이 해결하고 싶어하는 분야들에서 근면하고 책임적인 능력있는 개발자를 알게되는 행운을 나는 독특한 착상과 기술을 지닌, 개발자를 파악하고 등용할줄아는 관리형의 인재를 알게되는 행운을 지니게 될것입니다.  
Luck will begin with just one mouse click!!!
Our meeting will be the turning point of our destiny.
If you choose me, it may be an adventure too. But rest assured(Don't worry), your choice will be correct. By choosing me, you will be fortunate to meet a diligent, responsible, and capable developer in the fields you want to solve, and I will be fortunate to meet a managerial talent with unique ideas and skills who knows how to identify and hire developers.


--------------------------------------------------------------------------------------------
# About Me on Upwork
--------------------------------------------------------------------------------------------
# Main Profile
AI Engineer | RAG Expert | Full Stack Developer     $35.00/hr

𝐋𝐨𝐨𝐤𝐢𝐧𝐠 for an AI | Full Stack Engineer expert in LLM, Langchain, Vector Database, RAG, Automation, OpenAI, Python and GPT who provides Laser-Focused solutions and delivers on realistic timelines?

𝐌𝐲 𝐂𝐨𝐫𝐞 𝐒𝐤𝐢𝐥𝐥𝐬 & 𝐓𝐨𝐨𝐥𝐬:
𝐁𝐚𝐜𝐤𝐞𝐧𝐝: Node.js, Express, Django, Flask, Laravel, Core PHP
𝐅𝐫𝐨𝐧𝐭𝐞𝐧𝐝: React.js, Vue.js, Next.js, Nuxt.js
𝐃𝐚𝐭𝐚𝐛𝐚𝐬𝐞𝐬: MySQL, PostgreSQL, MongoDB, Supabase
Vector Database: Pinecone, Qdrant, Chroma, Milvus, Faiss
𝐀𝐏𝐈: RESTful APIs, Postman
𝐈𝐧𝐭𝐞𝐠𝐫𝐚𝐭𝐢𝐨𝐧𝐬: Payment Gateways, Stripe,Third-Party APIs, Third-Party Authentication (Google, Facebook, etc.)
𝐑𝐞𝐚𝐥-𝐓𝐢𝐦𝐞 𝐂𝐨𝐦𝐦𝐮𝐧𝐢𝐜𝐚𝐭𝐢𝐨𝐧: Pusher, Twilio, WebSockets
𝐕𝐞𝐫𝐬𝐢𝐨𝐧 𝐂𝐨𝐧𝐭𝐫𝐨𝐥 & 𝐃𝐞𝐩𝐥𝐨𝐲𝐦𝐞𝐧𝐭: Git, Docker
𝐂𝐥𝐨𝐮𝐝 & 𝐇𝐨𝐬𝐭𝐢𝐧𝐠: AWS, EC2 Instances
𝐀𝐈 & 𝐀𝐮𝐭𝐨𝐦𝐚𝐭𝐢𝐨𝐧: OpenAI, ChatGPT, LLM, LangChain

𝐒𝐨𝐟𝐭𝐰𝐚𝐫𝐞 𝐄𝐧𝐠𝐢𝐧𝐞𝐞𝐫𝐢𝐧𝐠 𝐄𝐱𝐩𝐞𝐫𝐭𝐢𝐬𝐞:
𝐀𝐠𝐢𝐥𝐞 𝐌𝐞𝐭𝐡𝐨𝐝𝐨𝐥𝐨𝐠𝐢𝐞𝐬: Proficient in Scrum frameworks for iterative and collaborative development.
𝐏𝐫𝐨𝐣𝐞𝐜𝐭 𝐌𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭: Skilled in JIRA and Trello for efficient task management and tracking.
𝐕𝐞𝐫𝐬𝐢𝐨𝐧 𝐂𝐨𝐧𝐭𝐫𝐨𝐥: Advanced use of GitHub, ensuring seamless versioning, branching, and collaboration.
𝐂𝐨𝐥𝐥𝐚𝐛𝐨𝐫𝐚𝐭𝐢𝐨𝐧 & 𝐖𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐚𝐭𝐢𝐨𝐧: Focused on enhancing team productivity through optimized processes and tools.

𝐀𝐛𝐨𝐮𝐭 𝐌𝐞:
Software Engineer with 7+ years of experience building 𝐒𝐚𝐚𝐒-𝐛𝐚𝐬𝐞𝐝 solutions. 𝐂𝐫𝐞𝐚𝐭𝐞𝐝 SaaS products, Ecommerce and Social Media Platforms. Passionate about leveraging React to deliver innovative and user-centric products.

Let’s collaborate to create exceptional, revenue-generating products—whether it’s for real estate, e-commerce, education, or public safety industries.


# Consultations
Create your first consultation
Meet more clients through 1-on-1 virtual consultations. Set your rate and choose when you're available to meet, then clients can book time with you. How it works
Link your calendar or share your availability.
No connects needed.
create a consultation

# Hours per week
More than 30 hrs/week
No contract-to-hire perference set



# Verifications
Military veteran

# Languages
English: Fluent
Ukrainian: Native or Bilingual


# Education
- Lviv polytechnic National University
Bachelor of Engineering
Computer science
2014-2019
- Lviv Polytechnic National University
Computer science
2014-2017


# Portfolio
contents: Markprompt for AI Agent
My role: AI Engineer and Backend developer

Project description
This project is for AI Agent to generate the chatbots that answer based on the data such as PDF, Docx, Git, Scarped websites, Slack and so on..
The chatbots can be integreate with various platform such as PHP, React, Svelt, Angular, Vue, and so on..
For this, I implemented RAG system using Pinecone(Vector Database), LLM(GPT model), and huggingface.

Skills and deliverables
Pinecone, ChatGPT, Hugging Face, Next.js, LangChain


# Wrok History
No items


# Skills
Retrieval Augmented Generation, Vector Database, LLM prompt Engineering,
Natural Language Processing, Pinecone, Machine Learning, Python, JavaScript, Vector Embedding,
React, Hugging Face, LangChain, AI Agent Development, OpenAI,  API, ChatBot

# Your project catalog
Projects are a new way to earn on upwork that helps you do more of the work you love to do.
Create project offerings that highlight your strengths and attact more clients.


# Testimonials
Endorsements from past clietns
Showcase your sills with non-upwork client testmonials.
Request a testimmonial


# Certifications
Listing your certifications can help prove your speific knowlegde orabilities.(+10%)
You can add them manually or import them from Credly.


# Emplyment History
- AI Engineer | TheGridBiz   (July 2022 - January 2024)
During my time at TheGridBiz, Leveraging my expertise in Generative AI, National Langue Process, Large Language Model and Vector Database. I optimized the accuracy of suggestions and analyzed results which will upload some social media. I also built a responsive and user-friendly Frontend that enabled users to access and interact easily with some companie's brands and services.

- Full Stack Developer | WamiSoftware  (April 2021 - May 2022)
During my time at WamiSoftware, I served as a Full Stack Developer and made significant contributions to the Asiansocial project. Leveraging my expertise in Javascript, Java, SprintBoot, React.js, and AWS.
I lead the development of frontend and backend and delivered successfully.

- Software Engineer | MindfulProd  (February 2020 - March 2021)
During my tenure at MindfulProd, I served as a software engineer and made significant contributions to the Telerad project. Leveraging my expertise in Javascript, Python, React.Js. I birdged the reporting gap between radiologists and doctors. I created a structure that enabled doctors to access patient results as DICOM files, write studies on the reports, and create studies for further anaysis.

--------------------------------------------------------------------------------------------
# Proposal History
--------------------------------------------------------------------------------------------

1. Client Name: `Mohammed N ( Mohammed Nehan )`
```
  We are seeking an experienced developer to build and implement AI multi-agent RAG (Retrieve and Generate) systems using LLMs, Langchain, and Vector DB technologies. The ideal candidate will have a strong background in AI and machine learning, along with practical experience in deploying scalable multi-agent systems. You will work closely with our team to integrate these systems into our existing infrastructure and enhance our capabilities. If you are passionate about innovative AI solutions and have a proven track record, we want to hear from you! 
```

- Cover letter
Hello, Hope you are going well.
Based on understand of your requirements, I will explain my previous project that very similar like your project.
### https://replient.ai
This project is for agencies automate comment management, maintain brand consistency, and increase engament - without the hassle on social platform such as Facebook, TikTok, Instagram, LinkedIn, etc.
1) For RAG system+, I utilized the Pinecone as Vector Database to retrive the relevant sections based on the data or information.
- Which the vector database should we choose?
I think there are various the vector database such as Pinecone, Qdrant, ChromaDB, and Supabase, but we can use one according to the requirements of the project.
In my project, I used the Pinecone and Supabase extension(Supabase provides extension to store embedded data in field in the table). The reason that I used the supabase is that it can be used for Backend, Authentification, and Vector DB.
- I implemented the features that get extract text and structure from the files such as Pdf, Docx, Excel, Html, and scraped websites.(To scrape the websites, I got the page lists from Robots.txt typically or used the Scrapy API when I can't scrape the websites).
- I implemented a chunking process, which is a critical component of RAG system where large datasets are divided into smaller, manageable pieces known as "chunks".
- I implemented the embedding process by converting each chunk info embeddings, using the OpenAI model(text-embedding-ada-002). (We can use also Huggingface).
- I designed the vector database. And I saved the chunks and their corresponding embedded data using a key(embedded data converted)-value(each chunk) strcture or other schemas.
2) User process.
In my project, I gathered user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API that Facebook provides.
- I converted the user's comments into embedded data to perform semantic searches on the vector database based on the uploaded data or information.
- After conducting the semantic search, I extracted the relevant sections(the divided chunks).
3) Prompt Design
I believe it is crucial to formulate effective prompts to obtain the desired results from LLM.(OpenAI, Gemini, Perplexity, depending on your preference).
- Base prompt: This is the prompt designed to eclicit the desired results from the provided data and user's input
- Context: This prompt combines the extracted chunks to provide necessary information.
- User prompt: This is the prompt that user inputs(in my project, it is user's comments).
- Final prompt: Combined with Base prompt, Context, User prompt, and others. (In my project, I used GPT-3.5-turbo, GPT-4 models)
It is different how can make the prompt according to developers, but in my case, I made the prompt is like here:
For example:
BASE PROMPT:
You are company agency to reply to the users on Facebook about our company....
You should answer based on
CONTEXT START:
{{CONTEXT}}
CONTEXT END
....
The comments of the user is {{USER INPUT}}
...
** The sentence to get the result we desired from LLM
4) Subscription:
I have integrated Stripe to handle subscription management.
#### https://markprompt.com
I developed MVP for this project before 2 years.
For now, It includes many agents such as Email, chatbots, Voice, Routing, Agent Assit, Reporting, knowledge, QA & compliance.
At first time, I developed the feature is focused in the chatbots.
Framework: Next JS
Backend & Auth & Vector Database: Supabase
Used the Langchain to manage the LLM and Vector Database.
The logic is same with repliet.ai project, and differences are I added the Langchan, and used the Supabase as Vector Database for RAG.
I am sure I can help you with my expertise and experiences.
Regards.


2. Client Name: unknown
```
LLM Chatbot - Questions about Manufacturing Data
Machine Learning Posted May 29, 2025
Our clients have a bunch of manufacturing data and they want to ask questions about their data and converse with a chatbot to get more insights about it. We have the data schema and could build a sandbox environment for python and/or SQL to run queries that will answer the client's questions and format the information correctly.
More info attached.
```
Hello, Hope you are going well.
Based on understand of your requirements, I will explain my previous project that very similar like your project.
### https://replient.ai
This project is for agencies automate comment management, maintain brand consistency, and increase engament - without the hassle on social platform such as Facebook, TikTok, Instagram, LinkedIn, etc.
1) For RAG system+, I utilized the Pinecone as Vector Database to retrive the relevant sections based on the data or information.
- Which the vector database should we choose?
I think there are various the vector database such as Pinecone, Qdrant, ChromaDB, and Supabase, but we can use one according to the requirements of the project.
In my project, I used the Pinecone and Supabase extension(Supabase provides extension to store embedded data in field in the table). The reason that I used the supabase is that it can be used for Backend, Authentification, and Vector DB.
- I implemented the features that get extract text and structure from the files such as Pdf, Docx, Excel, Html, and scraped websites.(To scrape the websites, I got the page lists from Robots.txt typically or used the Scrapy API when I can't scrape the websites).
- I implemented a chunking process, which is a critical component of RAG system where large datasets are divided into smaller, manageable pieces known as "chunks".
- I implemented the embedding process by converting each chunk info embeddings, using the OpenAI model(text-embedding-ada-002). (We can use also Huggingface).
- I designed the vector database. And I saved the chunks and their corresponding embedded data using a key(embedded data converted)-value(each chunk) strcture or other schemas.
2) User process.
In my project, I gathered user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API that Facebook provides.
- I converted the user's comments into embedded data to perform semantic searches on the vector database based on the uploaded data or information.
- After conducting the semantic search, I extracted the relevant sections(the divided chunks).
3) Prompt Design
I believe it is crucial to formulate effective prompts to obtain the desired results from LLM.(OpenAI, Gemini, Perplexity, depending on your preference).
- Base prompt: This is the prompt designed to eclicit the desired results from the provided data and user's input
- Context: This prompt combines the extracted chunks to provide necessary information.
- User prompt: This is the prompt that user inputs(in my project, it is user's comments).
- Final prompt: Combined with Base prompt, Context, User prompt, and others. (In my project, I used GPT-3.5-turbo, GPT-4 models)
It is different how can make the prompt according to developers, but in my case, I made the prompt is like here:
For example:
BASE PROMPT:
You are company agency to reply to the users on Facebook about our company....
You should answer based on
CONTEXT START:
{{CONTEXT}}
CONTEXT END
....
The comments of the user is {{USER INPUT}}
...
** The sentence to get the result we desired from LLM
4) Subscription:
I have integrated Stripe to handle subscription management.

#### https://markprompt.com
I developed MVP for this project before 2 years.
For now, It includes many agents such as Email, chatbots, Voice, Routing, Agent Assit, Reporting, knowledge, QA & compliance.
At first time, I developed the feature is focused in the chatbots.
Framework: Next JS
Backend & Auth & Vector Database: Supabase
Used the Langchain to manage the LLM and Vector Database.
The logic is same with repliet.ai project, and differences are I added the Langchan, and used the Supabase as Vector Database for RAG.

I am sure I can help you w ith my expertises and rich experiences.
Regards.

I'm looking for bids for the whole project, not hourly and not for only a part of the project. Is your bid a one time cost for the whole project outlined in the description including development and testing?
Understand. I agree.
Include a link to your GitHub profile and/or website
newangry


3. Client Name: unknown

```
Backend Integration for AI Bot Development
We are looking for a skilled developer to assist with backend integration for our AI bot project. The ideal candidate will have experience in building and integrating APIs, as well as a solid understanding of AI technologies. You will work closely with our team to ensure seamless communication between the bot and backend services. Your expertise will help enhance the bot's functionality and user experience. If you are passionate about AI and backend development, we would love to hear from you
```

Hello.
Hope you a re going well.
If you want to build AI bot, I think we use the RAG system.
Based on understand of your requirements, I will explain my previous project that very similar like your project.
### https://replient.ai
This project is for agencies automate comment management, maintain brand consistency, and increase engament - without the hassle on social platform such as Facebook, TikTok, Instagram, LinkedIn, etc.
1) For RAG system+, I utilized the Pinecone as Vector Database to retrive the relevant sections based on the data or information.
- Which the vector database should we choose?
I think there are various the vector database such as Pinecone, Qdrant, ChromaDB, and Supabase, but we can use one according to the requirements of the project.
In my project, I used the Pinecone and Supabase extension(Supabase provides extension to store embedded data in field in the table). The reason that I used the supabase is that it can be used for Backend, Authentification, and Vector DB.
- I implemented the features that get extract text and structure from the files such as Pdf, Docx, Excel, Html, and scraped websites.(To scrape the websites, I got the page lists from Robots.txt typically or used the Scrapy API when I can't scrape the websites).
- I implemented a chunking process, which is a critical component of RAG system where large datasets are divided into smaller, manageable pieces known as "chunks".
- I implemented the embedding process by converting each chunk info embeddings, using the OpenAI model(text-embedding-ada-002). (We can use also Huggingface).
- I designed the vector database. And I saved the chunks and their corresponding embedded data using a key(embedded data converted)-value(each chunk) strcture or other schemas.
2) User process.
In my project, I gathered user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API that Facebook provides.
- I converted the user's comments into embedded data to perform semantic searches on the vector database based on the uploaded data or information.
- After conducting the semantic search, I extracted the relevant sections(the divided chunks).
3) Prompt Design
I believe it is crucial to formulate effective prompts to obtain the desired results from LLM.(OpenAI, Gemini, Perplexity, depending on your preference).
- Base prompt: This is the prompt designed to eclicit the desired results from the provided data and user's input
- Context: This prompt combines the extracted chunks to provide necessary information.
- User prompt: This is the prompt that user inputs(in my project, it is user's comments).
- Final prompt: Combined with Base prompt, Context, User prompt, and others. (In my project, I used GPT-3.5-turbo, GPT-4 models)
It is different how can make the prompt according to developers, but in my case, I made the prompt is like here:
For example:
BASE PROMPT:
You are company agency to reply to the users on Facebook about our company....
You should answer based on
CONTEXT START:
{{CONTEXT}}
CONTEXT END
....
The comments of the user is {{USER INPUT}}
...
** The sentence to get the result we desired from LLM
4) Subscription:
I have integrated Stripe to handle subscription management.

#### https://markprompt.com
I developed MVP for this project before 2 years.
For now, It includes many agents such as Email, chatbots, Voice, Routing, Agent Assit, Reporting, knowledge, QA & compliance.
At first time, I developed the feature is focused in the chatbots.
Framework: Next JS
Backend & Auth & Vector Database: Supabase
Used the Langchain to manage the LLM and Vector Database.
The logic is same with repliet.ai project, and differences are I added the Langchan, and used the Supabase as Vector Database for RAG.

I can help your team with my expertise and experiences.
Regards.

Include a link to your GitHub profile and/or website
newangry
What frameworks have you worked with?
Next Js For frontend
Python(FastAPI) for backend
Describe your recent experience with similar projects
### https://replient.ai
This project is for agencies automate comment management, maintain brand consistency, and increase engament - without the hassle on social platform such as Facebook, TikTok, Instagram, LinkedIn, etc.
1) For RAG system+, I utilized the Pinecone as Vector Database to retrive the relevant sections based on the data or information.
- Which the vector database should we choose?
I think there are various the vector database such as Pinecone, Qdrant, ChromaDB, and Supabase, but we can use one according to the requirements of the project.
In my project, I used the Pinecone and Supabase extension(Supabase provides extension to store embedded data in field in the table). The reason that I used the supabase is that it can be used for Backend, Authentification, and Vector DB.
- I implemented the features that get extract text and structure from the files such as Pdf, Docx, Excel, Html, and scraped websites.(To scrape the websites, I got the page lists from Robots.txt typically or used the Scrapy API when I can't scrape the websites).
- I implemented a chunking process, which is a critical component of RAG system where large datasets are divided into smaller, manageable pieces known as "chunks".
- I implemented the embedding process by converting each chunk info embeddings, using the OpenAI model(text-embedding-ada-002). (We can use also Huggingface).
- I designed the vector database. And I saved the chunks and their corresponding embedded data using a key(embedded data converted)-value(each chunk) strcture or other schemas.
2) User process.
In my project, I gathered user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API that Facebook provides.
- I converted the user's comments into embedded data to perform semantic searches on the vector database based on the uploaded data or information.
- After conducting the semantic search, I extracted the relevant sections(the divided chunks).
3) Prompt Design
I believe it is crucial to formulate effective prompts to obtain the desired results from LLM.(OpenAI, Gemini, Perplexity, depending on your preference).
- Base prompt: This is the prompt designed to eclicit the desired results from the provided data and user's input
- Context: This prompt combines the extracted chunks to provide necessary information.
- User prompt: This is the prompt that user inputs(in my project, it is user's comments).
- Final prompt: Combined with Base prompt, Context, User prompt, and others. (In my project, I used GPT-3.5-turbo, GPT-4 models)
It is different how can make the prompt according to developers, but in my case, I made the prompt is like here:
For example:
BASE PROMPT:
You are company agency to reply to the users on Facebook about our company....
You should answer based on
CONTEXT START:
{{CONTEXT}}
CONTEXT END
....
The comments of the user is {{USER INPUT}}
...
** The sentence to get the result we desired from LLM
4) Subscription:
I have integrated Stripe to handle subscription management.

#### https://markprompt.com
I developed MVP for this project before 2 years.
For now, It includes many agents such as Email, chatbots, Voice, Routing, Agent Assit, Reporting, knowledge, QA & compliance.
At first time, I developed the feature is focused in the chatbots.
Framework: Next JS
Backend & Auth & Vector Database: Supabase
Used the Langchain to manage the LLM and Vector Database.
The logic is same with repliet.ai project, and differences are I added the Langchan, and used the Supabase as Vector Database for RAG



4. Client Name: unknown
Job details
```
Full Stack Developer for AI-Enabled Web & Mobile App | React, Firebase, Chatbot, Webflow
Full Stack Development Posted Jun 3, 2025
Project Scope

I'm representing an AI-first digital platform aimed at helping users interact more intelligently with everyday tools. This project blends intuitive design with smart automation—think conversational bots, AI-powered suggestions, and seamless mobile/web performance.

What You'll Be Doing

- Translating Figma designs into smooth, responsive web and mobile interfaces using React, Webflow, or Framer
- Developing backend systems with Firebase and Node.js to support scalable feature sets
- Building AI features such as chatbots, contextual search, and form automation
- Integrating third-party APIs for advanced user flows and data handling
- Creating fluid user experiences with clean UI and intuitive interactions
- Collaborating with the team to test and refine AI-driven UX components

Who You Are

You’re a full stack developer with a strong grasp of both frontend and backend workflows—and a genuine interest in AI-driven solutions. You’ve built clean interfaces and know how to layer in smart, helpful features that make the app feel one step ahead of the user.

If you’ve worked on products that used AI to enhance user engagement, search, automation, or support, we’d love to hear from you

```

# It is me..


Cover Letter
🍀 Luck will begin with just one mouse click 🎯
Our meeting might be a turning point in our destiny...
If you choose me, it may be an adventure too. 
Don't worry, your choice will be correct. 
By choosing me, you will be fortunate to meet a diligent, responsible and capable developer in the fields you want to solve and I will be fortunate to meet a managerial talent with unique ideas and skills who knows how to identify and hire developers.
🧠 Let's discuss the assignment...
I previously specialized in developing products that leverage AI to enhance user engagement, search, automation, or support, and AI ChatBot.
So, I have already experienced this type of work and have proven assets.
I have also experienced numerous projects using React.js and Node.js, etc.
But, It is the most important to resolve AI-related problems in this project. 
For example, In your project, If you are looking to build an AI bot, RAG System would be a good choice.
After understanding your requirements, I will tell you about my previous project that is very similar to yours.

### https://replient.ai
This project is for agencies that struggle to automate comment management, maintain brand consistency and increase engagement on social platforms such as Facebook, TikTok, Instagram and LinkedIn.

1) Using Pinecone as a vector database in RAG System+ to retrieve relevant sections based on data or information.
- Which vector database should I choose? There are many vector databases like Pinecone, Qdrant, ChromaDB, Supabase, etc., but you can use one according to your project requirements. In my project, I used Pinecone and Supabase extension (Supabase provides an extension to store data contained in table fields). The reason I used supabase is because it can be used for backend, authentication, and vector DB.
- Implemented the ability to extract text and structure from PDF, Docx, Excel, HTML files, and scraped websites. (Website scraping usually gets the list of pages from Robots.txt file, or if website scraping is not possible, uses Scrapy API.)
- Implemented the chunking process, which is a core component of RAG system. Chunking is the process of dividing a large dataset into small, manageable pieces called "chunks". - Implemented an embedding process that converts each chunk information into an embedding using the OpenAI model (text-embedding-ada-002). (You can also use Huggingface.)
- Designed a vector database. Stored chunks and embedding data using a key (converted embedding data)-value (each chunk) structure or other schemas.
2) User Process.
In this project, we collected user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API provided by Facebook.
- Converted user comments into embedding data, and performed semantic search on the vector database based on the uploaded data or information.
- After semantic search, extract relevant sections (segmented chunks).
3) Prompt Design
I think it is important to construct effective prompts to get the desired results from LLM (OpenAI, Gemini, Perplexity, etc. depending on user preference). - Base Prompt: A prompt designed to derive the desired result from the provided data and user input. - Context: A prompt that combines the extracted chunks to provide the required information.
- User Prompt: A prompt that the user types (in my project, it is the user's comment).
- Final Prompt: A combination of Base Prompt, Context, and User Prompt. (In my project, I used GPT-3.5-turbo, GPT-4 models.)
Every developer has a different way of creating prompts, but here's how I did it:
Example:
Base Prompt:
I am a businessperson who is replying to a company-related user on Facebook...
My reply should be based on:
Start Context:
{{context}}
End Context
...
User comment is {{user input}}
...
** Sentences to get the desired result in LLM
4) Subscriptions:
I integrated Stripe for subscription management.

### https://markprompt.com
I developed the MVP (optimized product) of this project 2 years ago. Currently, it includes various agents such as email, chatbot, voice, routing, agent support, reporting, knowledge, QA, compliance, etc.
Initially, we focused on developing features utilizing chatbot. Framework: Next JS
Backend and authentication, vector database: Supabase
We used Langchain to manage LLM and vector database. The logic is the same as repliet.ai project, but the difference is that we added Langchan and used Supabase as vector database for RAG.
...
Sorry, I am afraid I have told you too much and you may be tired.

I will help your team with my expertise and experience.
I will always be responsible for my results and strive to provide the best satisfaction to you. I sincerely hope that even if you don't choose me, you will find a capable and experienced developer and that this project will be a success.
But if you give me a job, I will do my best to live up to your expectations
Thank you for your time
.Take Care, Good luck.






hello?
Adam hasn't verified his freelancer yet, what's going on? Sorry, but one thing I'm worried about is that verify upwork in 3 days when a jobs were contracted. Can you guarantee the deadline?




# second my cover letter

Do you love your mother? 😃
Probably. I also respect and love my mother very much. My mother always tells me that people should be diligent, honest, and make good choices.
Isn't life a series of choices? Choice itself is also an adventure.

If you choose me, it may be an adventure too. But rest assured(Don't worry), your choice will be correct. By choosing me, you will be fortunate to meet a diligent, responsible, and capable developer in the fields you want to solve, and I will be fortunate to meet a managerial talent with unique ideas and skills who knows how to identify and hire developers.

They say that coincidence is a bridge that fate gives to those who try hard. Our meeting might be a turning point in our destiny...
Since I am new to you, I will be fine with a reasonable salary. I would be grateful if you would pay me more in the future according to my abilities.

I will help you with my expertise and experience.
I will always be responsible for my results and strive to provide the best satisfaction to you. I sincerely hope that even if you don't choose me, you will find a capable and experienced developer and that this project will be a success.
Thank you for your time.
Take Care


🧠 Let's discuss the assignment...
I have previously specialized in developing many AI products, including AI chatbots, using the RAG system. So, I have already experienced this type of work and have proven assets.
After understanding your requirements, I will tell you about my previous project that is very similar to yours.
### https://replient.ai
This project is for agencies that struggle to automate comment management, maintain brand consistency and increase engagement on social platforms such as Facebook, TikTok, Instagram and LinkedIn.

1) Using Pinecone as a vector database in RAG System+ to retrieve relevant sections based on data or information.
✔ Which vector database should I choose? There are many vector databases like Pinecone, Qdrant, ChromaDB, Supabase, etc., but you can use one according to your project requirements. In my project, I used Pinecone and Supabase extension (Supabase provides an extension to store data contained in table fields). The reason I used supabase is because it can be used for backend, authentication, and vector DB.
✔ Implemented the ability to extract text and structure from PDF, Docx, Excel, HTML files, and scraped websites. (Website scraping usually gets the list of pages from Robots.txt file, or if website scraping is not possible, uses Scrapy API.)
✔ Implemented the chunking process, which is a core component of RAG system. Chunking is the process of dividing a large dataset into small, manageable pieces called "chunks". - Implemented an embedding process that converts each chunk information into an embedding using the OpenAI model (text-embedding-ada-002). (You can also use Huggingface.)
✔ Designed a vector database. Stored chunks and embedding data using a key (converted embedding data)-value (each chunk) structure or other schemas.
2) User Process.
In this project, we collected user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API provided by Facebook.
✔ Converted user comments into embedding data, and performed semantic search on the vector database based on the uploaded data or information.
✔ After semantic search, extract relevant sections (segmented chunks).
3) Prompt Design
I think it is important to construct effective prompts to get the desired results from LLM (OpenAI, Gemini, Perplexity, etc. depending on user preference). - Base Prompt: A prompt designed to derive the desired result from the provided data and user input. - Context: A prompt that combines the extracted chunks to provide the required information.
✔ User Prompt: A prompt that the user types (in my project, it is the user's comment).
✔ Final Prompt: A combination of Base Prompt, Context, and User Prompt. (In my project, I used GPT-3.5-turbo, GPT-4 models.)
Every developer has a different way of creating prompts, but here's how I did it:
Example:
Base Prompt:
I am a businessperson who is replying to a company-related user on Facebook...
My reply should be based on:
Start Context:
{{context}}
End Context
...
User comment is {{user input}}
...
** Sentences to get the desired result in LLM
4) Subscriptions:
I integrated Stripe for subscription management.

### https://markprompt.com
I developed the MVP (optimized product) of this project 2 years ago. Currently, it includes various agents such as email, chatbot, voice, routing, agent support, reporting, knowledge, QA, compliance, etc.
Initially, we focused on developing features utilizing chatbot. Framework: Next JS
Backend and authentication, vector database: Supabase
We used Langchain to manage LLM and vector database. The logic is the same as repliet.ai project, but the difference is that we added Langchan and used Supabase as vector database for RAG.
...
Sorry, I am afraid I have told you too much and you may be tired.
I will send you more specific details via message.





Today..  python, Django, next.js, python framework, vue.js,  react.native.js... typescript

typing.. 
------------------------------------------------
step 1: syntax 
step 2: a sentence is self translate and compare (stack, toutorial, w3schools)
step 3: learn react-native
step 4: realtime check upwork and then bid... 
step 5: check github project  expensify
step 6: check gibhub md file
step 7: check Upwork with AI assist

check tutorial point job and best practise

/* ---------------------------
        This week target
--------------------------- */
Number1: Must Get Job on Upwork.!
Number2: Typing Level up ( at least 30mins)
Number3: English Level up - Grammar ( at least 1hour  )
Number4: English Level up - Bid Sentence ... ( at least 1hours)
Number5: English Level up - Words ( at least 30mins)
Number6:  Learn My Profile & Other Profile
---
Number7:  Upwork Usage & Concept 
Number8:  Get Dae-Hyok's upwork experience data .
Number9:  Study AI RAG System ...

Number10: Cleanup My Internet Book. ok? yes...

* like expensify,,, search on upwork, extra niche gigs!!!



# googling sentences:
The Hard Truth about upwork: 
How to get your first job on upwork(Fast) step by step
How to make it on upwork: Earn 5000+ monthly (in 2025)
Upwork proposal Tutorial For Beginners: the complete upwork guide
How to make money on upwork in 2025 (for beginners)












🍀 Luck will begin with just one mouse click 🎯
After understanding your requirements, I will tell you about my previous projects that are very similar to your project.
First of all, I would like to tell you that it is very reasonable to use the RAG system in your project development, and you can get competitive and excellent results by using the RAG system. I promise you that.
### https://replient.ai
This project is for agencies automate comment management, maintain brand consistency, and increase engament - without the hassle on social platform such as Facebook, TikTok, Instagram, LinkedIn, etc.
1) For RAG system+, I utilized the Pinecone as Vector Database to retrive the relevant sections based on the data or information.
- Which the vector database should we choose?
I think there are various the vector database such as Pinecone, Qdrant, ChromaDB, and Supabase, but we can use one according to the requirements of the project.
In my project, I used the Pinecone and Supabase extension(Supabase provides extension to store embedded data in field in the table). The reason that I used the supabase is that it can be used for Backend, Authentification, and Vector DB.
- I implemented the features that get extract text and structure from the files such as Pdf, Docx, Excel, Html, and scraped websites.(To scrape the websites, I got the page lists from Robots.txt typically or used the Scrapy API when I can't scrape the websites).
- I implemented a chunking process, which is a critical component of RAG system where large datasets are divided into smaller, manageable pieces known as "chunks".
- I implemented the embedding process by converting each chunk info embeddings, using the OpenAI model(text-embedding-ada-002). (We can use also Huggingface).
- I designed the vector database. And I saved the chunks and their corresponding embedded data using a key(embedded data converted)-value(each chunk) strcture or other schemas.
2) User process.
In my project, I gathered user comments from social platforms such as Facebook, LinkedIn, TikTok, and Instagram through the webhook API that Facebook provides.
- I converted the user's comments into embedded data to perform semantic searches on the vector database based on the uploaded data or information.
- After conducting the semantic search, I extracted the relevant sections(the divided chunks).
3) Prompt Design
I believe it is crucial to formulate effective prompts to obtain the desired results from LLM.(OpenAI, Gemini, Perplexity, depending on your preference).
- Base prompt: This is the prompt designed to eclicit the desired results from the provided data and user's input
- Context: This prompt combines the extracted chunks to provide necessary information.
- User prompt: This is the prompt that user inputs(in my project, it is user's comments).
- Final prompt: Combined with Base prompt, Context, User prompt, and others. (In my project, I used GPT-3.5-turbo, GPT-4 models)
It is different how can make the prompt according to developers, but in my case, I made the prompt is like here:
For example:
BASE PROMPT:
You are company agency to reply to the users on Facebook about our company....
You should answer based on
CONTEXT START:
{{CONTEXT}}
CONTEXT END
....
The comments of the user is {{USER INPUT}}
...
** The sentence to get the result we desired from LLM
4) Subscription:
I have integrated Stripe to handle subscription management.
#### https://markprompt.com
I developed MVP for this project before 2 years.
For now, It includes many agents such as Email, chatbots, Voice, Routing, Agent Assit, Reporting, knowledge, QA & compliance.
At first time, I developed the feature is focused in the chatbots.
Framework: Next JS
Backend & Auth & Vector Database: Supabase
Used the Langchain to manage the LLM and Vector Database.
The logic is same with repliet.ai project, and differences are I added the Langchan, and used the Supabase as Vector Database for RAG.

Our meeting might be a turning point in our destiny...
If you choose me, it may be an adventure too. 
Don't worry, your choice will be correct. 
By choosing me, you will be fortunate to meet a diligent, responsible and capable developer in the fields you want to solve and I will be fortunate to meet a managerial talent with unique ideas and skills who knows how to identify and hire developers.

I am sure I can help you with my expertise and experiences.
Regards.













# freelancer profile
🧠 AI Developer | GPT, LangChain, RAG, Vector DB | Python & JS
Hi, I'm a developer from Ukraine, working full-time as a freelancer.
In the past few years, I’ve been focusing on AI and LLM-based projects — especially tools that combine OpenAI, LangChain, vector databases, and automation.

I’ve helped build:

🔹 RAG systems for document Q&A (PDFs, web pages, Notion)

🔹 Internal AI assistants using LangChain + Pinecone or Qdrant

🔹 Custom GPT-based tools with real backend/frontend logic

Before switching fully to AI work, I spent years building SaaS platforms, admin dashboards, and API backends using JavaScript and Python. That full-stack background helps me create full products — not just prototypes.

⚙️ My Main Stack:
AI / LLM / Automation:
🔹 OpenAI (GPT-4, 3.5)
🔹 LangChain
🔹 Pinecone, Qdrant, Chroma, Faiss
🔹 RAG (Retrieval-Augmented Generation)
🔹 Prompt Engineering & Automation

Backend & APIs:
Node.js, Express, Django, Flask, Laravel, Core PHP
REST APIs, OAuth (Google, Facebook), Stripe, Twilio

Frontend:
React.js, Next.js, Vue.js, Nuxt.js

Databases & Hosting:
PostgreSQL, MySQL, MongoDB, Supabase
Docker, Git, EC2 (AWS), Postman, Trello

I’m open to freelance work, part-time tasks, or long-term projects.
If your idea is clear, I’ll deliver it fast. If something’s unclear, I’ll ask and move forward quickly — no waiting around.

Thanks for checking out my profile.







🔧 Full-Stack & AI Developer | GPT, LangChain, Vector DB, Automation
Hello! I’m a full-stack developer from Ukraine with 7+ years of experience. I’ve worked on many projects — from SaaS and eCommerce to automation tools and AI chatbots using GPT and vector databases.

In recent years, I started focusing more on AI/LLM projects like RAG systems, document search, chatbot integration, and workflow automation using LangChain and OpenAI.

I’m comfortable with both backend and frontend, and I like to keep things clean and fast.










---

current bid -> gpt -> 우에 공유한 cover letter는 제가 지금까지 몇달동안 RAG system혹은 AI chatbot과제에 제출한 cover letter입니다. 그런데 안타깝게도 고객들이 얼마간 읽어 보기는 하나 한번도 고용되지 못하였습니다. 어째서 고객들이 읽어보고도 저를 고용하지 않는지. 당신의 견해를 듣고 싶습니다. 만약 수정해야 한다면  고객이 나를 꼭 고용해주었으면 하는 나의 심정과 신중하고 사려깊은 요청의 마음 그리고 나의 강점과 진심, 겸손하고 솔직한 어조, 높은 책임성을 반영하여 cover letter를 수정해주었으면 하는것입니다. 또한 cover letter의 첫 두줄이 중요하다고 하던데 시작문장을 잘 만들어 주십시오. 강조하고 싶은것은 실지 개발자가 자기손으로 쓴것처럼 느껴져야 한다는것입니다. AI가 쓴것으로 느껴져서는 안됩니다. 부탁합니다.






























Adam Savinkin 
Principal Data Engineering Consultant at Time2Mobile | I help to achieve business goals by developing software solutions and providing teams of developers

Time2Mobile

Ivan Franko National University of Lviv




Excuse me, but is Adam's LinkedIn profile accurate? Is he really that great of a developer?
It's truly amazing. Can I use the exact profile he wrote?



